"""
Financial Agent Unified MCP Server
ç»Ÿä¸€çš„è´¢åŠ¡åŠ©æ‰‹ MCP æœåŠ¡å™¨ - åŒ…å«å¯¹è´¦å’Œæ•°æ®æ•´ç†åŠŸèƒ½
"""
import sys
import asyncio
from pathlib import Path
from mcp import types
from mcp.server import Server
from mcp.server.sse import SseServerTransport
from starlette.applications import Starlette
from starlette.routing import Route, Mount
from starlette.responses import Response, JSONResponse, FileResponse
from starlette.staticfiles import StaticFiles
import uvicorn
import logging

# å¯¼å…¥å®‰å…¨å·¥å…·
from security_utils import validate_task_id, sanitize_path

# å¯¼å…¥å¯¹è´¦æ¨¡å—
from reconciliation.mcp_server.config import DEFAULT_HOST, DEFAULT_PORT
from reconciliation.mcp_server.tools import create_tools as create_recon_tools, handle_tool_call as handle_recon_call

# å¯¼å…¥æ•°æ®æ•´ç†æ¨¡å—
from data_preparation.mcp_server.tools import create_tools as create_prep_tools, handle_tool_call as handle_prep_call
from data_preparation.mcp_server.config import OUTPUT_DIR, REPORT_DIR as PREP_REPORT_DIR_IMPORT

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºç»Ÿä¸€çš„ MCP Server
mcp_server = Server("financial-mcp-server")


@mcp_server.list_tools()
async def list_tools() -> list[types.Tool]:
    """åˆ—å‡ºæ‰€æœ‰å·¥å…·ï¼ˆå¯¹è´¦ + æ•°æ®æ•´ç†ï¼‰"""
    try:
        recon_tools = create_recon_tools()
        logger.info(f"å¯¹è´¦å·¥å…·æ•°é‡: {len(recon_tools)}")
    except Exception as e:
        logger.error(f"åŠ è½½å¯¹è´¦å·¥å…·å¤±è´¥: {str(e)}", exc_info=True)
        recon_tools = []
    
    try:
        prep_tools = create_prep_tools()
        logger.info(f"æ•°æ®æ•´ç†å·¥å…·æ•°é‡: {len(prep_tools)}")
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ•´ç†å·¥å…·å¤±è´¥: {str(e)}", exc_info=True)
        prep_tools = []
    
    all_tools = recon_tools + prep_tools
    logger.info(f"æ€»å·¥å…·æ•°é‡: {len(all_tools)}")
    return all_tools


@mcp_server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:
    """è°ƒç”¨å·¥å…·ï¼ˆè‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”æ¨¡å—ï¼‰"""
    try:
        # æ ¹æ®å·¥å…·åå‰ç¼€è·¯ç”±åˆ°å¯¹åº”æ¨¡å—
        # å¯¹è´¦æ¨¡å—å·¥å…·ï¼šä»¥ reconciliation_ å¼€å¤´ï¼Œæˆ–è€… file_uploadã€get_reconciliation
        if name.startswith("reconciliation_") or name in ["file_upload", "get_reconciliation"]:
            result = await handle_recon_call(name, arguments)
        elif name.startswith("data_preparation_"):
            result = await handle_prep_call(name, arguments)
        else:
            result = {"error": f"æœªçŸ¥çš„å·¥å…·: {name}"}
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        import json
        result_str = json.dumps(result, ensure_ascii=False, indent=2)
        
        return [types.TextContent(type="text", text=result_str)]
    
    except Exception as e:
        error_msg = f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return [types.TextContent(type="text", text=error_msg)]


# åˆ›å»º SSE Transport
sse_transport = SseServerTransport("/messages/")


# HTTP ç«¯ç‚¹å¤„ç†å‡½æ•°
async def handle_sse(request):
    """å¤„ç† SSE è¿æ¥"""
    async with sse_transport.connect_sse(request.scope, request.receive, request._send) as streams:
        await mcp_server.run(
            streams[0],
            streams[1],
            mcp_server.create_initialization_options()
        )


async def health_check(request):
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse({
        "status": "healthy",
        "service": "financial-mcp-server",
        "version": "1.0.0",
        "modules": ["reconciliation", "data_preparation"]
    })


async def download_file(request):
    """æ–‡ä»¶ä¸‹è½½ç«¯ç‚¹"""
    task_id = request.path_params.get("task_id")
    logger.info(f"ä¸‹è½½æ–‡ä»¶è¯·æ±‚: task_id={task_id}")

    # éªŒè¯ task_id æ ¼å¼ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    if not validate_task_id(task_id):
        logger.warning(f"éæ³•çš„ task_id: {task_id}")
        return JSONResponse(
            {"error": "æ— æ•ˆçš„ä»»åŠ¡IDæ ¼å¼"},
            status_code=400
        )

    # å…ˆæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
    from data_preparation.mcp_server.task_manager import TaskManager
    from data_preparation.mcp_server.config import (
        UPLOAD_DIR as PREP_UPLOAD_DIR,
        OUTPUT_DIR as PREP_OUTPUT_DIR,
        REPORT_DIR as PREP_REPORT_DIR,
        DATA_PREPARATION_SCHEMAS_FILE
    )

    # å°è¯•ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ç»“æœ
    task_manager = TaskManager()
    try:
        result = await task_manager.get_task_result(task_id)
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        result = None

    # ä»ä»»åŠ¡ç»“æœä¸­è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–å¤„ç†ID
    proc_id = None
    output_file = None
    
    if result and not result.get('error'):
        # å¦‚æœç»“æœä¸­æœ‰ output_fileï¼Œç›´æ¥ä½¿ç”¨
        if result.get('output_file'):
            output_file = sanitize_path(PREP_OUTPUT_DIR, Path(result['output_file']).name)
        # å¦‚æœç»“æœä¸­æœ‰ task_idï¼ˆå¯èƒ½æ˜¯å¤„ç†IDï¼‰ï¼Œç”¨äºæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶
        proc_id = result.get('task_id') or task_id
    
    # å¦‚æœæ‰¾ä¸åˆ°è¾“å‡ºæ–‡ä»¶ï¼Œå°è¯•ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è·å–
    if (not output_file or not output_file.exists()) and proc_id:
        # å…ˆå°è¯•ä½¿ç”¨å¤„ç†IDæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶
        report_file = sanitize_path(PREP_REPORT_DIR, f"{proc_id}_report.json")
        if not report_file or not report_file.exists():
            # å¦‚æœå¤„ç†IDæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨ä»»åŠ¡ID
            report_file = sanitize_path(PREP_REPORT_DIR, f"{task_id}_report.json")
        
        if report_file and report_file.exists():
            import json
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                    # å°è¯•ä» processing_steps ä¸­è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
                    processing_steps = report_data.get('processing_steps', [])
                    if processing_steps:
                        last_step = processing_steps[-1]
                        output_path = last_step.get('details', {}).get('output_file')
                        if output_path:
                            output_file = sanitize_path(PREP_OUTPUT_DIR, Path(output_path).name)
                    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æ ¹çº§åˆ«è·å–
                    if (not output_file or not output_file.exists()) and report_data.get('output_file'):
                        output_file = sanitize_path(PREP_OUTPUT_DIR, Path(report_data['output_file']).name)
            except (json.JSONDecodeError, IndexError, KeyError) as e:
                logger.error(f"è§£ææŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                # ç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•ï¼Œä¸ç›´æ¥è¿”å›é”™è¯¯

    # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾åŒ…å«ä»»åŠ¡IDæˆ–å¤„ç†IDçš„æ–‡ä»¶
    if not output_file or not output_file.exists():
        try:
            output_files = list(PREP_OUTPUT_DIR.glob("*"))
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            if output_files:
                output_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                # å¦‚æœä»»åŠ¡IDæˆ–å¤„ç†IDåœ¨æ–‡ä»¶åä¸­ï¼Œä¼˜å…ˆé€‰æ‹©
                for f in output_files:
                    if task_id in f.name or (proc_id and proc_id in f.name):
                        output_file = f
                        break
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
                if (not output_file or not output_file.exists()) and output_files:
                    output_file = output_files[0]
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)

    if output_file and output_file.exists():
        logger.info(f"æ‰¾åˆ°è¾“å‡ºæ–‡ä»¶: {output_file}")
        return FileResponse(
            str(output_file),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=output_file.name
        )

    # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    logger.warning(f"æœªæ‰¾åˆ°æ–‡ä»¶: task_id={task_id}, proc_id={proc_id}, output_file={output_file}")
    return JSONResponse({
        "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {task_id}",
        "message": "è¯·ç¡®è®¤ä»»åŠ¡å·²å®Œæˆä¸”è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆ",
        "task_id": task_id
    }, status_code=404)


async def preview_file(request):
    """æ–‡ä»¶é¢„è§ˆç«¯ç‚¹ï¼ˆè¿”å›æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ï¼‰"""
    task_id = request.path_params.get("task_id")

    # éªŒè¯ task_id æ ¼å¼ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    if not validate_task_id(task_id):
        logger.warning(f"éæ³•çš„ task_id: {task_id}")
        return JSONResponse(
            {"error": "æ— æ•ˆçš„ä»»åŠ¡IDæ ¼å¼"},
            status_code=400
        )

    # å…ˆæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
    from data_preparation.mcp_server.task_manager import TaskManager
    from data_preparation.mcp_server.config import (
        OUTPUT_DIR as PREP_OUTPUT_DIR,
        REPORT_DIR as PREP_REPORT_DIR
    )

    # å°è¯•ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ç»“æœ
    task_manager = TaskManager()
    try:
        result = await task_manager.get_task_result(task_id)
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        result = None

    # ä»ä»»åŠ¡ç»“æœä¸­è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–å¤„ç†ID
    proc_id = None
    output_file = None
    
    if result and not result.get('error'):
        # å¦‚æœç»“æœä¸­æœ‰ output_fileï¼Œç›´æ¥ä½¿ç”¨
        if result.get('output_file'):
            output_file = sanitize_path(PREP_OUTPUT_DIR, Path(result['output_file']).name)
        # å¦‚æœç»“æœä¸­æœ‰ task_idï¼ˆå¯èƒ½æ˜¯å¤„ç†IDï¼‰ï¼Œç”¨äºæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶
        proc_id = result.get('task_id') or task_id

    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è·å–
    if (not output_file or not output_file.exists()) and proc_id:
        # å…ˆå°è¯•ä½¿ç”¨å¤„ç†IDæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶
        report_file = sanitize_path(PREP_REPORT_DIR, f"{proc_id}_report.json")
        if not report_file or not report_file.exists():
            # å¦‚æœå¤„ç†IDæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨ä»»åŠ¡ID
            report_file = sanitize_path(PREP_REPORT_DIR, f"{task_id}_report.json")
        
        if report_file and report_file.exists():
            import json
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                    # å°è¯•ä» processing_steps ä¸­è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
                    processing_steps = report_data.get('processing_steps', [])
                    if processing_steps:
                        last_step = processing_steps[-1]
                        output_path = last_step.get('details', {}).get('output_file')
                        if output_path:
                            output_file = sanitize_path(PREP_OUTPUT_DIR, Path(output_path).name)
                    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æ ¹çº§åˆ«è·å–
                    if (not output_file or not output_file.exists()) and report_data.get('output_file'):
                        output_file = sanitize_path(PREP_OUTPUT_DIR, Path(report_data['output_file']).name)
            except (json.JSONDecodeError, IndexError, KeyError) as e:
                logger.error(f"è§£ææŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                # ç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•ï¼Œä¸ç›´æ¥è¿”å›é”™è¯¯

    # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾åŒ…å«ä»»åŠ¡IDæˆ–å¤„ç†IDçš„æ–‡ä»¶
    if not output_file or not output_file.exists():
        try:
            output_files = list(PREP_OUTPUT_DIR.glob("*"))
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            if output_files:
                output_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                # å¦‚æœä»»åŠ¡IDæˆ–å¤„ç†IDåœ¨æ–‡ä»¶åä¸­ï¼Œä¼˜å…ˆé€‰æ‹©
                for f in output_files:
                    if task_id in f.name or (proc_id and proc_id in f.name):
                        output_file = f
                        break
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
                if (not output_file or not output_file.exists()) and output_files:
                    output_file = output_files[0]
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)

    if not output_file or not output_file.exists():
        return JSONResponse({"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {task_id}"}, status_code=404)

    import openpyxl
    try:
        wb = openpyxl.load_workbook(str(output_file), read_only=True, data_only=True)
        sheets = []
        for sheet_name in wb.sheetnames:
            ws = wb[sheet_name]
            sheets.append({
                "name": sheet_name,
                "rows": ws.max_row,
                "columns": ws.max_column
            })

        return JSONResponse({
            "filename": output_file.name,
            "size": output_file.stat().st_size,
            "sheets": sheets,
            "download_url": f"/download/{task_id}"
        })
    except Exception as e:
        logger.error(f"é¢„è§ˆæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return JSONResponse({"error": f"é¢„è§ˆå¤±è´¥: {str(e)}"}, status_code=500)


async def get_report(request):
    """è·å–è¯¦ç»†æŠ¥å‘Š"""
    task_id = request.path_params.get("task_id")

    # éªŒè¯ task_id æ ¼å¼ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    if not validate_task_id(task_id):
        logger.warning(f"éæ³•çš„ task_id: {task_id}")
        return JSONResponse(
            {"error": "æ— æ•ˆçš„ä»»åŠ¡IDæ ¼å¼"},
            status_code=400
        )

    from data_preparation.mcp_server.task_manager import TaskManager
    from data_preparation.mcp_server.config import (
        REPORT_DIR as PREP_REPORT_DIR,
        OUTPUT_DIR as PREP_OUTPUT_DIR
    )

    logger.info(f"è·å–æŠ¥å‘Šè¯·æ±‚: task_id={task_id}")

    # å°è¯•ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ç»“æœï¼Œä»¥è·å–å¤„ç†ID
    task_manager = TaskManager()
    proc_id = None
    try:
        result = await task_manager.get_task_result(task_id)
        if result and not result.get('error'):
            # å¦‚æœç»“æœä¸­æœ‰ task_idï¼ˆå¯èƒ½æ˜¯å¤„ç†IDï¼‰ï¼Œç”¨äºæŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶
            proc_id = result.get('task_id') or task_id
            logger.info(f"ä»ä»»åŠ¡ç®¡ç†å™¨è·å–å¤„ç†ID: {proc_id}")
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        result = None

    # æŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶çš„å¤šç§ç­–ç•¥
    report_file = None
    
    # ç­–ç•¥1: ä½¿ç”¨å¤„ç†IDæŸ¥æ‰¾
    if proc_id and proc_id != task_id:
        report_file = sanitize_path(PREP_REPORT_DIR, f"{proc_id}_report.json")
        if report_file and report_file.exists():
            logger.info(f"ä½¿ç”¨å¤„ç†IDæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶: {report_file}")
    
    # ç­–ç•¥2: ä½¿ç”¨ä»»åŠ¡IDæŸ¥æ‰¾
    if not report_file or not report_file.exists():
        report_file = sanitize_path(PREP_REPORT_DIR, f"{task_id}_report.json")
        if report_file and report_file.exists():
            logger.info(f"ä½¿ç”¨ä»»åŠ¡IDæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶: {report_file}")
    
    # ç­–ç•¥3: ä»è¾“å‡ºæ–‡ä»¶æ¨æ–­å¤„ç†IDï¼ˆå¦‚æœè¾“å‡ºæ–‡ä»¶ååŒ…å«æ—¥æœŸæ—¶é—´æˆ³ï¼‰
    if not report_file or not report_file.exists():
        try:
            # æŸ¥æ‰¾è¾“å‡ºç›®å½•ä¸­å¯èƒ½ç›¸å…³çš„æ–‡ä»¶
            output_files = list(PREP_OUTPUT_DIR.glob("*"))
            if output_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                output_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                # å°è¯•ä»æœ€æ–°çš„è¾“å‡ºæ–‡ä»¶åä¸­æå–æ—¥æœŸæ—¶é—´ä¿¡æ¯
                for f in output_files:
                    import re
                    # æ–‡ä»¶åæ ¼å¼å¯èƒ½æ˜¯: è´§å¸èµ„é‡‘å®¡è®¡åº•ç¨¿_20260116_163437.xlsx
                    date_match = re.search(r'(\d{8})_(\d{6})', f.name)
                    if date_match:
                        date_str = date_match.group(1)
                        time_str = date_match.group(2)
                        # æ„å»ºå¯èƒ½çš„å¤„ç†IDå‰ç¼€: proc_20260116_163437_
                        proc_prefix = f"proc_{date_str}_{time_str}"
                        logger.info(f"ä»æ–‡ä»¶åæ¨æ–­å¤„ç†IDå‰ç¼€: {proc_prefix}")
                        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æŠ¥å‘Šæ–‡ä»¶
                        matching_reports = list(PREP_REPORT_DIR.glob(f"{proc_prefix}*_report.json"))
                        if matching_reports:
                            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                            matching_reports.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                            report_file = matching_reports[0]
                            logger.info(f"ä»è¾“å‡ºæ–‡ä»¶æ¨æ–­æ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶: {report_file}")
                            break
        except Exception as e:
            logger.error(f"ä»è¾“å‡ºæ–‡ä»¶æ¨æ–­å¤„ç†IDå¤±è´¥: {str(e)}", exc_info=True)
    
    # ç­–ç•¥4: åˆ—å‡ºæ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    if not report_file or not report_file.exists():
        try:
            report_files = list(PREP_REPORT_DIR.glob("*_report.json"))
            if report_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                report_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                # å¦‚æœä»»åŠ¡IDåœ¨æ–‡ä»¶åä¸­ï¼Œä¼˜å…ˆé€‰æ‹©
                for f in report_files:
                    if task_id in f.name:
                        report_file = f
                        logger.info(f"ä»æŠ¥å‘Šæ–‡ä»¶åˆ—è¡¨ä¸­æ‰¾åˆ°åŒ¹é…çš„æŠ¥å‘Š: {report_file}")
                        break
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
                if (not report_file or not report_file.exists()) and report_files:
                    report_file = report_files[0]
                    logger.info(f"ä½¿ç”¨æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        except Exception as e:
            logger.error(f"åˆ—å‡ºæŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)

    if not report_file or not report_file.exists():
        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æŠ¥å‘Šæ–‡ä»¶ä¾›å‚è€ƒ
        try:
            available_reports = [f.name for f in PREP_REPORT_DIR.glob("*_report.json")]
            available_reports.sort(reverse=True)  # æŒ‰åç§°å€’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            logger.warning(f"æœªæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶ï¼Œå¯ç”¨æŠ¥å‘Š: {available_reports[:5]}")
            return JSONResponse({
                "error": f"æŠ¥å‘Šä¸å­˜åœ¨: {task_id}",
                "message": "è¯·ç¡®è®¤ä»»åŠ¡å·²å®Œæˆä¸”æŠ¥å‘Šæ–‡ä»¶å·²ç”Ÿæˆ",
                "available_reports": available_reports[:10] if available_reports else [],
                "suggestion": "å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œå¯ä»¥å°è¯•ä½¿ç”¨å¤„ç†IDï¼ˆproc_xxxï¼‰è®¿é—®æŠ¥å‘Š"
            }, status_code=404)
        except Exception as e:
            logger.error(f"åˆ—å‡ºå¯ç”¨æŠ¥å‘Šå¤±è´¥: {str(e)}", exc_info=True)
            return JSONResponse({
                "error": f"æŠ¥å‘Šä¸å­˜åœ¨: {task_id}",
                "message": "è¯·ç¡®è®¤ä»»åŠ¡å·²å®Œæˆä¸”æŠ¥å‘Šæ–‡ä»¶å·²ç”Ÿæˆ"
            }, status_code=404)

    try:
        import json
        with open(report_file, 'r', encoding='utf-8') as f:
            report_data = json.load(f)
        return JSONResponse(report_data)
    except json.JSONDecodeError as e:
        logger.error(f"æŠ¥å‘Šæ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}", exc_info=True)
        return JSONResponse({"error": "æŠ¥å‘Šæ–‡ä»¶æ ¼å¼é”™è¯¯"}, status_code=500)
    except Exception as e:
        logger.error(f"è¯»å–æŠ¥å‘Šå¤±è´¥: {str(e)}", exc_info=True)
        return JSONResponse({"error": f"è¯»å–æŠ¥å‘Šå¤±è´¥: {str(e)}"}, status_code=500)


# è·¯ç”±é…ç½®
routes = [
    Route("/sse", endpoint=handle_sse, methods=["GET", "POST"]),
    Route("/mcp", endpoint=handle_sse, methods=["GET", "POST"]),
    Mount("/messages/", app=sse_transport.handle_post_message),
    Route("/health", endpoint=health_check),
    Route("/download/{task_id}", endpoint=download_file),
    Route("/preview/{task_id}", endpoint=preview_file),
    Route("/report/{task_id}", endpoint=get_report),
]

app = Starlette(routes=routes)


async def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    host = DEFAULT_HOST
    port = DEFAULT_PORT
    
    # åŠ¨æ€è·å–å·¥å…·åˆ—è¡¨ç”¨äºæ˜¾ç¤º
    try:
        tools = await list_tools()
        recon_tools = [t for t in tools if t.name.startswith("reconciliation_") or t.name == "file_upload" or t.name == "get_reconciliation"]
        prep_tools = [t for t in tools if t.name.startswith("data_preparation_")]
    except Exception as e:
        logger.warning(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
        recon_tools = []
        prep_tools = []
    
    recon_tools_text = "\n".join([f"  {i+1}. {t.name:<30} - {t.description}" for i, t in enumerate(recon_tools)])
    prep_tools_text = "\n".join([f"  {len(recon_tools)+i+1}. {t.name:<30} - {t.description}" for i, t in enumerate(prep_tools)])
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          Financial Agent MCP Server å¯åŠ¨ä¸­...                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ æœåŠ¡ç«¯ç‚¹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  â€¢ SSE ç«¯ç‚¹:        http://{host}:{port}/sse
  â€¢ æ¶ˆæ¯ç«¯ç‚¹:        http://{host}:{port}/messages/
  â€¢ å¥åº·æ£€æŸ¥:        http://{host}:{port}/health
  â€¢ æ–‡ä»¶ä¸‹è½½:        http://{host}:{port}/download/{{task_id}}
  â€¢ æ–‡ä»¶é¢„è§ˆ:        http://{host}:{port}/preview/{{task_id}}
  â€¢ è¯¦ç»†æŠ¥å‘Š:        http://{host}:{port}/report/{{task_id}}

ğŸ› ï¸  å¯ç”¨å·¥å…·ï¼ˆå¯¹è´¦æ¨¡å—ï¼Œ{len(recon_tools)}ä¸ªï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{recon_tools_text}

ğŸ› ï¸  å¯ç”¨å·¥å…·ï¼ˆæ•°æ®æ•´ç†æ¨¡å—ï¼Œ{len(prep_tools)}ä¸ªï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{prep_tools_text}

ğŸ“– ä½¿ç”¨è¯´æ˜:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  åœ¨ Dify ä¸­é…ç½®:
    MCP æœåŠ¡å™¨åœ°å€: http://localhost:{port}/sse
    æˆ–ä½¿ç”¨ Docker:   http://host.docker.internal:{port}/sse

æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ...
""")
    
    config = uvicorn.Config(
        app,
        host=host,
        port=port,
        log_level="info"
    )
    server = uvicorn.Server(config)
    await server.serve()


if __name__ == "__main__":
    asyncio.run(main())
